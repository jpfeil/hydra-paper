{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import shutil\n",
    "import getpass\n",
    "import multiprocessing\n",
    "import copy\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "from threading import Timer\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from scipy.stats import random_correlation\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomialP\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "date = str(datetime.date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "CPU = min(15, multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global models\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = '../data/gencode.v22.annotation.gene.probeMap'\n",
    "probemap = pd.read_csv(pth, sep='\\t')\n",
    "genemap = probemap[[\"id\", 'gene']].set_index(\"id\").to_dict()[\"gene\"]\n",
    "probemap[\"length\"] = probemap[\"chromEnd\"] - probemap[\"chromStart\"]\n",
    "genelengths = probemap.groupby(\"gene\").mean()[\"length\"]\n",
    "genekb = genelengths / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = '../data/TARGET-WT.htseq_counts.tsv.gz'\n",
    "lc1 = pd.read_csv(pth, sep='\\t', index_col=0)\n",
    "lc1 = (2**lc1) - 1\n",
    "\n",
    "lc1 = lc1.reset_index()\n",
    "lc1[\"hugo\"] = lc1[\"Ensembl_ID\"].map(genemap).values\n",
    "lc1 = lc1.dropna()\n",
    "lc1 = lc1.drop(\"Ensembl_ID\", axis=1)\n",
    "exp = lc1.groupby(\"hugo\").sum()\n",
    "thresh80 = np.percentile(exp.mean(axis=1).sort_values(ascending=False).values, 80)\n",
    "exp = exp[exp.mean(axis=1) > thresh80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb(y):\n",
    "    x = np.ones_like(y)\n",
    "    res = sm.NegativeBinomial(y, x, loglike_method='nb2').fit(disp=0)\n",
    "    \n",
    "    mu = np.exp(res.params[0])\n",
    "    alpha = res.params[1]\n",
    "    size = 1. / alpha\n",
    "    prob = size / (size + mu)\n",
    "    return stats.nbinom(size, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno    \n",
    "import os\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = '../data/h.all.v6.2.symbols.gmt'\n",
    "mapper = {}\n",
    "gs_genes = set()\n",
    "with open(pth) as f:\n",
    "    for line in f:\n",
    "        elements = line.strip().split('\\t')\n",
    "        name = elements[0]\n",
    "        if 'HALLMARK' in name:\n",
    "            desc = elements[1]\n",
    "            genes = elements[2:]\n",
    "            mapper[name] = [g for g in genes if g in exp.index.values]\n",
    "            gs_genes.update(mapper[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohend(d1, d2):\n",
    "    \"\"\"\n",
    "    https://machinelearningmastery.com/effect-size-measures-in-python/\n",
    "    \"\"\"\n",
    "    # calculate the size of samples\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    # calculate the variance of the samples\n",
    "    s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    "    # calculate the pooled standard deviation\n",
    "    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    # calculate the means of the samples\n",
    "    u1, u2 = np.mean(d1), np.mean(d2)\n",
    "    # calculate the effect size\n",
    "    return (u1 - u2) / s\n",
    "\n",
    "def run(cmd, timeout_sec=900):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/1191374/using-module-subprocess-with-timeout\n",
    "    :param cmd:\n",
    "    :param timeout_sec:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    proc = Popen(cmd, stdout=PIPE, stderr=PIPE)\n",
    "    timer = Timer(timeout_sec, proc.kill)\n",
    "\n",
    "    try:\n",
    "        timer.start()\n",
    "        stdout, stderr = proc.communicate()\n",
    "\n",
    "    finally:\n",
    "        timer.cancel()\n",
    "\n",
    "    return stdout, stderr\n",
    "\n",
    "def run_hydra(data_pth, gs, database, output):\n",
    "    cmd = ['docker',\n",
    "           'run',\n",
    "           '-v', '%s:/data' % os.getcwd(),\n",
    "           'jpfeil/hydra@sha256:de39cd5b3b04b4ff87e8751084a21e743f25af431048bddbfe2faded05268467',\n",
    "           'sweep',\n",
    "           '-e', data_pth,\n",
    "           '--gmt', database,\n",
    "           '--gmt-regex', gs,\n",
    "           '--min-prob-filter', '0.05',\n",
    "           '--min-mean-filter', '1.0',\n",
    "           '--min-gene-filter', '1',\n",
    "           '--sensitive',\n",
    "           '--gamma', '5.00',\n",
    "           '--sF', '2.0',\n",
    "           '-K', '2',\n",
    "           '--CPU', str(CPU),\n",
    "           '--debug',\n",
    "           '--output-dir', output]\n",
    "    \n",
    "    stdout, stderr = run(cmd)\n",
    "    \n",
    "    assert os.path.exists(output)\n",
    "    \n",
    "    return stdout, stderr\n",
    "    \n",
    "def run_gsea(data_pth, output_pth, database):\n",
    "    cmd = ['docker',\n",
    "           'run',\n",
    "           '-v', '%s:/data' % os.getcwd(),\n",
    "           'jpfeil/pyrwrapper@sha256:a765a8f284c94b7791cfe87b8b83b34594a609873bca755b2fb83e45b5dd56fc',\n",
    "           'GSEA-manager.py',\n",
    "           data_pth, \n",
    "           output_pth,\n",
    "           database]\n",
    "    \n",
    "    stdout, stderr = run(cmd)\n",
    "    assert os.path.exists(output_pth)\n",
    "    return stdout, stderr\n",
    "\n",
    "\n",
    "def AlmightyCorrcoefEinsumOptimized(O, P):\n",
    "    \"\"\"\n",
    "    https://github.com/ikizhvatov/efficient-columnwise-correlation/blob/master/columnwise_corrcoef_perf.py\n",
    "    \"\"\"\n",
    "    (n, t) = O.shape      # n traces of t samples\n",
    "    (n_bis, m) = P.shape  # n predictions for each of m candidates\n",
    "\n",
    "    DO = O - (np.einsum(\"nt->t\", O, optimize='optimal') / np.double(n)) # compute O - mean(O)\n",
    "    DP = P - (np.einsum(\"nm->m\", P, optimize='optimal') / np.double(n)) # compute P - mean(P)\n",
    "\n",
    "    cov = np.einsum(\"nm,nt->mt\", DP, DO, optimize='optimal')\n",
    "\n",
    "    varP = np.einsum(\"nm,nm->m\", DP, DP, optimize='optimal')\n",
    "    varO = np.einsum(\"nt,nt->t\", DO, DO, optimize='optimal')\n",
    "    tmp = np.einsum(\"m,t->mt\", varP, varO, optimize='optimal')\n",
    "    return cov / np.sqrt(tmp)\n",
    "    \n",
    "    \n",
    "def corr2cov(corr, std):\n",
    "    return corr * np.outer(std, std)\n",
    "\n",
    "def get_log2tpm1(og, mod):\n",
    "    denom = genekb.reindex(og.index)\n",
    "    rpk = og.divide( denom, axis=0 )\n",
    "    scale = rpk.sum(axis=0) / 1000000.\n",
    "    \n",
    "    mod_rpk = mod.divide( denom, axis=0 )\n",
    "    tpm = mod_rpk.divide(scale, axis=1)    \n",
    "    return np.log2( tpm + 1 )\n",
    "\n",
    "def get_model_callback(results):\n",
    "    models[results[0]] = results[1]\n",
    "\n",
    "def nb_runner(gene, exp):\n",
    "    return gene, get_nb(exp.loc[gene, :].values)\n",
    "    \n",
    "def get_data(models, targets, size, \n",
    "             eff, pdiff, ptype):\n",
    "    \n",
    "    print(\"Creating test data...\")\n",
    "    ssize = int(ptype * size)\n",
    "    bsize = size - ssize\n",
    "    \n",
    "    bsamples = ['normal%d' % x for x in range(bsize)]\n",
    "    ssamples = ['active%d' % x for x in range(bsize, size)]\n",
    "\n",
    "    samples = bsamples + ssamples\n",
    "    assert len(samples) == size\n",
    "    \n",
    "    genes = list(models.keys())\n",
    "        \n",
    "    train = pd.DataFrame(index=genes,\n",
    "                         columns=samples)\n",
    "        \n",
    "    test = pd.DataFrame(index=genes,\n",
    "                        columns=samples)\n",
    "    \n",
    "    print(\"Sampling from TRAIN and TEST distributions\")\n",
    "    for gene in genes:   \n",
    "        train.loc[gene, :] = models[gene].rvs(size)\n",
    "        test.loc[gene, :] = models[gene].rvs(size)\n",
    "        \n",
    "    og_train = copy.deepcopy(train)\n",
    "    og_test = copy.deepcopy(test)\n",
    "    \n",
    "    print(\"Randomly sampling DEGs\")\n",
    "    degs = np.random.choice(targets, \n",
    "                            int(pdiff * len(targets)), \n",
    "                            replace=False)\n",
    "    \n",
    "    print(\"Creating subtype mean expression vector\")\n",
    "    for i, gene in enumerate(degs):\n",
    "        mu = models[gene].mean()\n",
    "        s = models[gene].std()\n",
    "        \n",
    "        mu2 = (eff * s) + mu\n",
    "        \n",
    "        size = models[gene].args[0]\n",
    "        prob = models[gene].args[1]\n",
    "    \n",
    "        train.loc[gene, ssamples] = stats.nbinom(size, prob, loc=mu2).rvs(ssize)\n",
    "        test.loc[gene, ssamples] = stats.nbinom(size, prob, loc=mu2).rvs(ssize)\n",
    "      \n",
    "    train_log2TPM = get_log2tpm1(og_train, train)\n",
    "    test_log2TPM = get_log2tpm1(og_test, test)\n",
    "    \n",
    "    cds = []\n",
    "    for gene in degs:\n",
    "        cd = cohend(train_log2TPM.loc[gene, ssamples].values,\n",
    "                    train_log2TPM.loc[gene, bsamples].values)\n",
    "        cds.append(cd)\n",
    "        \n",
    "    return train_log2TPM, test_log2TPM, degs, np.mean(cds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(in_dir, out_dir, gs, gs_genes, data, \n",
    "             effect, percent_diff, models, size=300):\n",
    "    \n",
    "    # One criticism was that I picked too many magic \n",
    "    # numbers, so here I'm going to pick a random number\n",
    "    subtype_frac = round(random.uniform(0.15, 0.4), 2)\n",
    "    \n",
    "    tag = 'eff-%.2f-diff-%.2f-frac-%.2f-size-%d' % (effect, \n",
    "                                                    percent_diff, \n",
    "                                                    subtype_frac, \n",
    "                                                    size)\n",
    "    print(tag)\n",
    "    \n",
    "    train_pth = os.path.join(in_dir, \n",
    "                             'synthetic-%s-train-%s-%s.tsv' % (gs, \n",
    "                                                               tag, \n",
    "                                                               date))\n",
    "    \n",
    "    test_pth = os.path.join(in_dir, \n",
    "                            'synthetic-%s-test-%s-%s.tsv' % (gs, \n",
    "                                                             tag, \n",
    "                                                             date))\n",
    "    \n",
    "    \n",
    "    deg_pth = os.path.join(in_dir,\n",
    "                           'synthetic-%s-degs-%s-%s.tsv' % (gs, \n",
    "                                                            tag,\n",
    "                                                            date))\n",
    "    \n",
    "    cohen_pth = os.path.join(in_dir, \n",
    "                             'synthetic-%s-cohen-%s-%s.tsv' % (gs, \n",
    "                                                               tag,\n",
    "                                                               date))\n",
    "    \n",
    "    \n",
    "    train, test, degs, cohen = get_data(models, \n",
    "                                        gs_genes,\n",
    "                                        size, \n",
    "                                        effect, \n",
    "                                        percent_diff, \n",
    "                                        subtype_frac)\n",
    "    \n",
    "        \n",
    "    train.to_csv(train_pth, sep='\\t')\n",
    "    test.to_csv(test_pth, sep='\\t')\n",
    "    \n",
    "    with open(deg_pth, 'w') as f:\n",
    "        f.write('\\n'.join(degs))\n",
    "        \n",
    "    with open(cohen_pth, 'w') as f:\n",
    "        f.write(str(cohen))\n",
    "    \n",
    "    hydra_pth = os.path.join(out_dir, 'Hydra', tag, gs)\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    stdout, stderr = run_hydra(train_pth, gs, '../data/h.all.v6.2.symbols.gmt', hydra_pth)\n",
    "    toc = time.perf_counter()\n",
    "    \n",
    "    time_dir = os.path.join(out_dir, 'TIME', 'Hydra', tag)\n",
    "    mkdir_p(time_dir)\n",
    "    time_pth = os.path.join(time_dir, gs)\n",
    "    with open(time_pth, 'w') as f:\n",
    "        f.write(str(toc - tic))\n",
    "    \n",
    "    gsea_pth = os.path.join(out_dir)\n",
    "    run_gsea(test_pth, gsea_pth, '../data/h.all.v6.2.symbols.gmt')\n",
    "    \n",
    "\n",
    "def run_validation(in_dir,\n",
    "                   out_dir,\n",
    "                   data, \n",
    "                   gene_sets, \n",
    "                   effects, \n",
    "                   percent_diffs,\n",
    "                   models,\n",
    "                   sizes):   \n",
    "    \n",
    "    for size in sizes:\n",
    "        for effect in effects:\n",
    "            for percent_diff in percent_diffs:\n",
    "                # Sample gene sets to save time\n",
    "                if len(gene_sets) > 10:\n",
    "                    gs_subsets = random.sample(gene_sets.keys(), k=10)\n",
    "            \n",
    "                else:\n",
    "                    gs_subsets = gene_sets.keys()\n",
    "                \n",
    "                for gs in gs_subsets:\n",
    "                    validate(in_dir, \n",
    "                             out_dir,\n",
    "                             gs, \n",
    "                             gene_sets[gs],\n",
    "                             data, \n",
    "                             effect, \n",
    "                             percent_diff,\n",
    "                             models,\n",
    "                             size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    in_dir = 'test_input/%s' % date\n",
    "    out_dir = 'test_output/%s' % date\n",
    "    \n",
    "    if os.path.exists(in_dir):\n",
    "        shutil.rmtree(in_dir)\n",
    "    if os.path.exists(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "    \n",
    "    mkdir_p(in_dir)\n",
    "    mkdir_p(out_dir)\n",
    "    \n",
    "    _exp = exp.reindex(mapper['HALLMARK_ESTROGEN_RESPONSE_EARLY']).dropna()\n",
    "    test = {'HALLMARK_ESTROGEN_RESPONSE_EARLY': [x for x in mapper['HALLMARK_ESTROGEN_RESPONSE_EARLY']]}\n",
    "    \n",
    "    models = {}\n",
    "    for gene in _exp.index.values:\n",
    "        models[gene] = get_nb(_exp.loc[gene, :].values)\n",
    "    \n",
    "    print(\"Staring Run...\")\n",
    "    run_validation(in_dir,\n",
    "                   out_dir,\n",
    "                   _exp, \n",
    "                   test,\n",
    "                   [1.5],\n",
    "                   [0.10, 0.25], \n",
    "                   models,\n",
    "                   [300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real deal\n",
    "if not TEST:\n",
    "    in_dir = '../data/input/%s' % date\n",
    "    out_dir = '../data/output/%s' % date\n",
    "    mkdir_p(in_dir)\n",
    "    mkdir_p(out_dir)\n",
    "\n",
    "    with open(os.path.join(out_dir, 'V17'), 'a') as f:\n",
    "        f.write(date)\n",
    "\n",
    "    models = {}\n",
    "    for gene in exp.index.values:\n",
    "        models[gene] = get_nb(exp.loc[gene, :].values)\n",
    "          \n",
    "    assert len(models) == len(exp)\n",
    "    run_validation(in_dir, \n",
    "                   out_dir,\n",
    "                   exp, \n",
    "                   mapper,\n",
    "                   [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 2.5, 3.0],\n",
    "                   [0.25, 0.1],\n",
    "                   models,\n",
    "                   [50, 100, 200, 400, 800])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
